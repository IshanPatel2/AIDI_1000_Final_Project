{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e201b65",
   "metadata": {},
   "source": [
    "# <center>Text Classification Algorithms: A Survey</center>\n",
    "\n",
    "###### <center>Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura Barnes and Donald Brown.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a31aec",
   "metadata": {},
   "source": [
    "# <center>Logistic Regression:</center>\n",
    "\n",
    "#### A key approach for text classification is logistic regression, which uses input text data to forecast categorical outcomes. It works by simulating the likelihood that each class label will be connected to a certain text sample. The technique uses a logistic function to turn a linear combination of weighted data collected from the text into a probability value between 0 and 1.\n",
    "#### When classifying texts, characteristics frequently refer to the frequency or occurrence of words or n-grams (sequence word combinations). Each word is given a weight based on its frequency or presence, which shows how important it is for classifying entities. Utilizing optimization techniques, the model modifies these weights during training to reduce the discrepancy between expected and real labels. The logistic function is then suited for binary and multi-class classification tasks by mapping the weighted sum of features to a probability value.\n",
    "\n",
    "#### Research paper Link: <a href = \"https://arxiv.org/pdf/1904.08067v5.pdf\">Click Here</a>\n",
    "\n",
    "#### Dataset source link: <a href = \"https://github.com/kk7nc/Text_Classification/tree/master/Data\">Click Here</a>\n",
    "\n",
    "#### Github Link: <a href = \"https://github.com/kk7nc/Text_Classification/tree/master/code\">Click Here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9622b",
   "metadata": {},
   "source": [
    "#### Here, we performed the logistic regression algorithm on the same data from the <a href = \"https://github.com/kk7nc/Text_Classification/blob/master/Data/Download_Glove.py\">Glove</a> , which is different from the set algorithms performed by the authors of the research paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa3b60",
   "metadata": {},
   "source": [
    "#### The list of algorithm performed by the author of the paper are: <a href=\"https://github.com/kk7nc/Text_Classification/tree/master/code/RMDL\">click here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29a37e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=8, micro=8, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    " * Last Update: 04/25/2018\n",
    " * This file is part of  RMDL project, University of Virginia.\n",
    " * Free to use, change, share and distribute source code of RMDL\n",
    " * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n",
    " * Comments and Error: email: kk7nc@virginia.edu\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, tarfile\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib  # ugly but works\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "# image shape\n",
    "\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = '.\\Glove'\n",
    "\n",
    "# url of the binary data\n",
    "\n",
    "\n",
    "\n",
    "# path to the binary train file with image data\n",
    "\n",
    "\n",
    "def download_and_extract(data='Wikipedia'):\n",
    "    \"\"\"\n",
    "    Download and extract the GloVe\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    if data=='Wikipedia':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    elif data=='Common_Crawl_840B':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'\n",
    "    elif data=='Common_Crawl_42B':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'\n",
    "    elif data=='Twitter':\n",
    "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'\n",
    "    else:\n",
    "        print(\"prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia\")\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    print(filepath)\n",
    "\n",
    "    path = os.path.abspath(dest_directory)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath)#, reporthook=_progress)\n",
    "\n",
    "\n",
    "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "        zip_ref.close()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b320ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=8, micro=8, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n",
    " * Last Update: 04/25/2018\n",
    " * This file is part of  RMDL project, University of Virginia.\n",
    " * Free to use, change, share and distribute source code of RMDL\n",
    " * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n",
    " * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n",
    " * Comments and Error: email: kk7nc@virginia.edu\n",
    "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, tarfile\n",
    "import numpy as np\n",
    "\n",
    "if sys.version_info >= (3, 0, 0):\n",
    "    import urllib.request as urllib  # ugly but works\n",
    "else:\n",
    "    import urllib\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "# image shape\n",
    "\n",
    "\n",
    "# path to the directory with the data\n",
    "DATA_DIR = '.\\data_WOS'\n",
    "\n",
    "# url of the binary data\n",
    "DATA_URL = 'http://kowsari.net/WebOfScience.tar.gz'\n",
    "\n",
    "\n",
    "# path to the binary train file with image data\n",
    "\n",
    "\n",
    "def download_and_extract():\n",
    "    \"\"\"\n",
    "    Download and extract the WOS datasets\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dest_directory = DATA_DIR\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "\n",
    "\n",
    "    path = os.path.abspath(dest_directory)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
    "\n",
    "        print('Downloaded', filename)\n",
    "\n",
    "        tarfile.open(filepath, 'r').extractall(dest_directory)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3657d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: RMDL in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (1.20.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (1.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (1.6.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (2.7.0)\n",
      "Requirement already satisfied: keras>=2.0.9 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (0.24.1)\n",
      "Requirement already satisfied: nltk>=3.2.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from RMDL) (3.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.2->RMDL) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.2->RMDL) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.2->RMDL) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.2->RMDL) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from matplotlib>=2.1.2->RMDL) (2.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.2.4->RMDL) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.2.4->RMDL) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.2.4->RMDL) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.2.4->RMDL) (4.65.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->RMDL) (2021.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.0->RMDL) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (0.12.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (1.6.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (12.0.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (3.19.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (3.7.4.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (0.40.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (1.12.1)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (2.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (0.22.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow->RMDL) (1.41.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (2.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (3.3.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow->RMDL) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm->nltk>=3.2.4->RMDL) (0.4.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->RMDL) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->RMDL) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->RMDL) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->RMDL) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>='4.4' in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->RMDL) (3.10.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->RMDL) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->RMDL) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->RMDL) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->RMDL) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from importlib-metadata>='4.4'->markdown>=2.6.8->tensorboard~=2.6->tensorflow->RMDL) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->RMDL) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->RMDL) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras-Preprocessing in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Keras-Preprocessing) (1.20.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from Keras-Preprocessing) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install RMDL\n",
    "!pip install Keras-Preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dropout, Dense,Input,Embedding,Flatten, AveragePooling2D, Conv2D,Reshape\n",
    "from keras.models import Sequential,Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.utils import pad_sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea5246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(r\"C:\\Users\\ASUS\\Downloads\\glove.6B\\glove.6B.100d.txt\", encoding=\"utf8\") ## GloVe file which could be download https://nlp.stanford.edu/projects/glove/\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f181a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_logistic_regression(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=100):\n",
    "\n",
    "    model = Sequential()\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
    "                      \"into shape\", str(len(embedding_vector)), \"Please make sure your\"\n",
    "                      \"EMBEDDING_DIM is equal to the embedding_vector file, GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Flatten()(embedded_sequences)\n",
    "    preds = Dense(nclasses, activation='softmax')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1989a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=8, micro=8, releaselevel='final', serial=0)\n",
      "sys.version_info(major=3, minor=8, micro=8, releaselevel='final', serial=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5182 unique tokens.\n",
      "(80, 500)\n",
      "Total 400000 word vectors.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 100)          518300    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                1000020   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,518,320\n",
      "Trainable params: 1,518,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1/1 - 1s - loss: 3.0373 - accuracy: 0.1000 - val_loss: 16.4142 - val_accuracy: 0.0750 - 589ms/epoch - 589ms/step\n",
      "Epoch 2/20\n",
      "1/1 - 0s - loss: 4.5796 - accuracy: 0.6000 - val_loss: 24.0475 - val_accuracy: 0.0500 - 41ms/epoch - 41ms/step\n",
      "Epoch 3/20\n",
      "1/1 - 0s - loss: 5.8483 - accuracy: 0.6750 - val_loss: 28.9149 - val_accuracy: 0.0750 - 41ms/epoch - 41ms/step\n",
      "Epoch 4/20\n",
      "1/1 - 0s - loss: 7.0174 - accuracy: 0.6250 - val_loss: 31.7627 - val_accuracy: 0.0250 - 45ms/epoch - 45ms/step\n",
      "Epoch 5/20\n",
      "1/1 - 0s - loss: 6.6431 - accuracy: 0.6500 - val_loss: 36.3209 - val_accuracy: 0.0500 - 50ms/epoch - 50ms/step\n",
      "Epoch 6/20\n",
      "1/1 - 0s - loss: 7.2687 - accuracy: 0.5750 - val_loss: 30.4511 - val_accuracy: 0.0250 - 48ms/epoch - 48ms/step\n",
      "Epoch 7/20\n",
      "1/1 - 0s - loss: 3.4482 - accuracy: 0.7500 - val_loss: 29.7385 - val_accuracy: 0.0500 - 43ms/epoch - 43ms/step\n",
      "Epoch 8/20\n",
      "1/1 - 0s - loss: 2.1712 - accuracy: 0.7750 - val_loss: 26.8579 - val_accuracy: 0.0250 - 43ms/epoch - 43ms/step\n",
      "Epoch 9/20\n",
      "1/1 - 0s - loss: 1.3256 - accuracy: 0.9000 - val_loss: 24.9913 - val_accuracy: 0.1250 - 39ms/epoch - 39ms/step\n",
      "Epoch 10/20\n",
      "1/1 - 0s - loss: 0.6032 - accuracy: 0.8750 - val_loss: 24.7950 - val_accuracy: 0.1500 - 39ms/epoch - 39ms/step\n",
      "Epoch 11/20\n",
      "1/1 - 0s - loss: 0.3781 - accuracy: 0.9250 - val_loss: 28.3060 - val_accuracy: 0.1000 - 51ms/epoch - 51ms/step\n",
      "Epoch 12/20\n",
      "1/1 - 0s - loss: 0.5284 - accuracy: 0.9500 - val_loss: 32.1748 - val_accuracy: 0.0750 - 51ms/epoch - 51ms/step\n",
      "Epoch 13/20\n",
      "1/1 - 0s - loss: 0.9439 - accuracy: 0.9000 - val_loss: 34.4898 - val_accuracy: 0.0750 - 46ms/epoch - 46ms/step\n",
      "Epoch 14/20\n",
      "1/1 - 0s - loss: 1.0877 - accuracy: 0.9250 - val_loss: 35.3654 - val_accuracy: 0.0750 - 41ms/epoch - 41ms/step\n",
      "Epoch 15/20\n",
      "1/1 - 0s - loss: 0.9415 - accuracy: 0.9250 - val_loss: 35.3042 - val_accuracy: 0.1000 - 39ms/epoch - 39ms/step\n",
      "Epoch 16/20\n",
      "1/1 - 0s - loss: 0.5594 - accuracy: 0.9250 - val_loss: 35.3903 - val_accuracy: 0.0250 - 39ms/epoch - 39ms/step\n",
      "Epoch 17/20\n",
      "1/1 - 0s - loss: 0.2801 - accuracy: 0.9250 - val_loss: 35.4310 - val_accuracy: 0.0250 - 49ms/epoch - 49ms/step\n",
      "Epoch 18/20\n",
      "1/1 - 0s - loss: 0.0966 - accuracy: 0.9750 - val_loss: 35.0122 - val_accuracy: 0.0250 - 48ms/epoch - 48ms/step\n",
      "Epoch 19/20\n",
      "1/1 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 34.6899 - val_accuracy: 0.0250 - 45ms/epoch - 45ms/step\n",
      "Epoch 20/20\n",
      "1/1 - 0s - loss: 1.8890e-04 - accuracy: 1.0000 - val_loss: 34.6327 - val_accuracy: 0.0250 - 48ms/epoch - 48ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from RMDL import text_feature_extraction as txt\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data[:40]\n",
    "X_test = newsgroups_test.data[:40]\n",
    "y_train = newsgroups_train.target[:40]\n",
    "y_test = newsgroups_test.target[:40]\n",
    "\n",
    "\n",
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n",
    "\n",
    "model_LR = build_model_logistic_regression(word_index,embeddings_index, 20)\n",
    "\n",
    "\n",
    "model_LR.summary()\n",
    "\n",
    "model_LR.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=20,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_LR.predict(X_test_Glove)\n",
    "\n",
    "predicted = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57deccd5",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "Using GloVe embeddings and logistic regression to achieve text categorization, in conclusion, provides important new perspectives in the field of natural language processing. In the context of machine learning and NLP, the essential ideas of tokenization, embedding, and classification are laid forth in this project as a starting point.\n",
    "\n",
    "The model captures complex semantic linkages and contextual nuances found in the dataset by using the pre-trained GloVe word embeddings. For this binary classification task, the straightforward and effective logistic regression approach is a suitable option.\n",
    "\n",
    "Important tasks including data pre-processing, model creation, and evaluation are addressed throughout the implementation process. The model shows the importance of hyperparameter optimization in optimizing model performance in addition to achieving impressive accuracy of 1.0 in the last 2 epoch. \n",
    "\n",
    "As text classification is crucial for sentiment analysis, spam detection, and content categorization, the project also emphasizes the importance of real-world applicability. This practical experience not only improves understanding of machine learning methods, but also offers a foundation for investigating future developments in the constantly developing field of NLP.\n",
    "\n",
    "In essence, this code supports not only the research of the varied and potent applications of text categorization but also the practical grasp of logistic regression and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced554fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
